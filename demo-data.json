{
  "skills": [
    {
      "org": "engram-hq",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: engram-hq-org-knowledge\ndescription: |\n  Organization knowledge for the Engram platform - a Skill & Memory Browser for AI Agents.\n  Covers architecture, conventions, and cross-repo patterns.\nlast_updated: 2026-02-12\n---\n\n# Engram Platform Architecture\n\n## Overview\n\nEngram is a multi-tenant SaaS platform that indexes and visualizes AI agent skills and memories stored across GitHub repositories. It provides cost analytics for AI agent operations.\n\n## Repositories\n\n- **engram-web**: Next.js 16 web application + REST API (primary)\n- **.skills**: This repository - org-level knowledge base\n\n## Tech Stack\n\n- **Framework**: Next.js 16 (App Router) + React 19\n- **Database**: Turso (SQLite via libSQL) + Drizzle ORM\n- **Auth**: Auth.js v5 with GitHub OAuth + API key auth for agents\n- **UI**: shadcn/ui + Tailwind v4 + Recharts\n- **GitHub Integration**: Octokit REST API\n- **Markdown**: react-markdown v10 + rehype-highlight\n\n## Architecture Decisions\n\n### Dual Authentication\n- Web users: GitHub OAuth via Auth.js v5 \u2192 session cookies\n- AI agents: API keys with `eng_` prefix \u2192 Bearer token\n- `authenticateRequest()` checks Bearer first, session fallback\n\n### 3-Tier Skill Hierarchy\n1. **User level** (Tier 1): Cross-org skills in user's skills repo\n2. **Org level** (Tier 2): Org-specific skills in `<org>/.skills`\n3. **Repo level** (Tier 3): Project-specific skills in `<repo>/.skills/`\n\n### Database Design\n- 10 tables in Turso (SQLite)\n- Content hashing (SHA-256) for incremental sync\n- Daily rollup tables for fast analytics queries\n- FTS5 virtual tables for full-text search\n\n### Agent Metrics\n- POST /v1/metrics/ingest for single/batch event ingestion\n- Server-side cost calculation from model_pricing table\n- Daily rollups via upsert with atomic SQL increments\n\n## Conventions\n\n- All IDs are hex(randomblob(16))\n- API keys: `eng_` prefix + 32 random hex bytes, stored as SHA-256 hash\n- Tokens encrypted with AES-256-GCM (IV:authTag:ciphertext format)\n- Content hash: SHA-256 for change detection during sync\n- Memory types: session, cumulative, reference (inferred from path and content)\n\n## Known Issues\n\n- ReactMarkdown v10 removed className prop - wrap in div\n- Edge Runtime can't import Node.js crypto - handle auth in API routes\n- Auth.js v5 profile.id is string|null|undefined - use Number()\n",
      "chronoIndex": 4
    },
    {
      "org": "engram-hq",
      "repo": ".skills",
      "tier": 2,
      "path": "cost-tracking/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: session-cost-tracking\ndescription: |\n  Automatically extract and save AI agent cost metrics from Claude Code sessions\n  into session memory frontmatter. Enables real cost analytics in Engram.\n  MANDATORY: When creating or updating a session memory file, include cost metrics.\nlast_updated: 2026-02-12\n---\n\n# Session Cost Tracking\n\n## Purpose\n\nEvery session memory file MUST include real cost metrics in its YAML frontmatter.\nThis enables Engram to render accurate cost analytics from authentic data -\nno estimates, no assumptions, no fabricated numbers.\n\n## Required Frontmatter Fields\n\nWhen writing a session memory file (`<org>/.memory/sessions/YYYY-MM-DD-<slug>.md`),\ninclude these fields in the YAML frontmatter:\n\n```yaml\n---\ndate: \"2026-02-12\"\norg: my-org\nrepo: my-repo\ngoal: What this session accomplished\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 36533\noutput_tokens: 97287\ncache_read_tokens: 130057002\ncache_creation_tokens: 3267470\ncost_usd: 88.07\nduration_minutes: 180\napi_calls: 1320\n---\n```\n\n### Field Definitions\n\n| Field | Type | Source | Required |\n|-------|------|--------|----------|\n| `model` | string | `message.model` from session JSONL | Yes |\n| `input_tokens` | integer | Sum of `usage.input_tokens` across all assistant messages | Yes |\n| `output_tokens` | integer | Sum of `usage.output_tokens` across all assistant messages | Yes |\n| `cache_read_tokens` | integer | Sum of `usage.cache_read_input_tokens` | Yes |\n| `cache_creation_tokens` | integer | Sum of `usage.cache_creation_input_tokens` | Yes |\n| `cost_usd` | float | Calculated using pricing formula below | Yes |\n| `duration_minutes` | integer | Approximate session duration | Recommended |\n| `api_calls` | integer | Count of assistant messages in session | Recommended |\n\n## Data Sources\n\n### 1. Session JSONL Files (Primary)\n\nClaude Code stores every session at:\n```\n~/.claude/projects/<project-slug>/<session-id>.jsonl\n```\n\nEach line is a JSON record. Records with `type: \"assistant\"` contain:\n```json\n{\n  \"type\": \"assistant\",\n  \"message\": {\n    \"model\": \"claude-opus-4-6\",\n    \"usage\": {\n      \"input_tokens\": 36533,\n      \"output_tokens\": 97287,\n      \"cache_read_input_tokens\": 130057002,\n      \"cache_creation_input_tokens\": 3267470,\n      \"cache_creation\": {\n        \"ephemeral_5m_input_tokens\": 0,\n        \"ephemeral_1h_input_tokens\": 3267470\n      },\n      \"output_tokens\": 97287,\n      \"service_tier\": \"standard\"\n    }\n  }\n}\n```\n\nSum all `usage` fields across all assistant messages in the session to get totals.\n\n### 2. Stats Cache (Aggregate)\n\n`~/.claude/stats-cache.json` contains aggregate data:\n- `modelUsage`: Total tokens per model (all-time)\n- `dailyActivity`: Messages, sessions, tool calls per day\n- `dailyModelTokens`: Output tokens per model per day\n\n### 3. Anthropic Admin API (Organization-level)\n\nFor API key users (not Max plan), the Admin API provides server-side billing data:\n- `GET /v1/organizations/usage_report/messages` - token usage by day/model/key\n- `GET /v1/organizations/cost_report` - dollar costs by day/model/key\n\nRequires an Admin API key (`sk-ant-admin...`).\n\n## Cost Calculation Formula\n\n### Published Pricing (per million tokens, USD)\n\n| Model | Input | Output | Cache Read | Cache Write (5min) | Cache Write (1hr) |\n|-------|-------|--------|------------|--------------------|--------------------|\n| Claude Opus 4.6 | $5 | $25 | $0.50 | $6.25 | $10 |\n| Claude Opus 4.5 | $5 | $25 | $0.50 | $6.25 | $10 |\n| Claude Sonnet 4.5 | $3 | $15 | $0.30 | $3.75 | $6 |\n| Claude Haiku 4.5 | $1 | $5 | $0.10 | $1.25 | $2 |\n\n### Formula\n\n```python\n# Default: assume 5-minute ephemeral cache (most common in Claude Code)\ncost_usd = (\n    input_tokens * input_price +\n    output_tokens * output_price +\n    cache_read_tokens * cache_read_price +\n    cache_creation_tokens * cache_write_5m_price\n) / 1_000_000\n```\n\n### For Max Plan Users\n\nMax plan subscriptions report `costUSD: 0` in local stats. The formula above\ncalculates the **equivalent API cost** - what the session would cost at\nstandard API pricing. This is useful for:\n- Understanding relative session costs (which sessions are expensive)\n- Projecting costs for API-key-based deployments\n- Comparing model efficiency (Opus vs Sonnet for similar tasks)\n\nNote this in the memory frontmatter:\n```yaml\ncost_usd: 88.07  # equivalent API cost (Max plan subscriber)\n```\n\n## Projected Cost Calculation\n\nUse historical session data to project future costs:\n\n```python\navg_cost_per_session = total_cost / total_sessions\nsessions_per_day = total_sessions / active_days\nprojected_monthly = avg_cost_per_session * sessions_per_day * 30\nprojected_annual = projected_monthly * 12\n```\n\nInclude in analytics when sufficient data exists (>= 5 sessions).\n\n## How to Extract Data\n\n### Option A: Inline (during session)\n\nAt the end of a Claude Code session, when writing a session memory:\n\n1. Identify the current session JSONL file from `~/.claude/projects/`\n2. Parse all assistant messages, sum usage fields\n3. Apply pricing formula\n4. Include in frontmatter\n\n### Option B: Helper Script\n\nRun `extract-session-costs.py` (in this skill directory) to retroactively\nextract cost data from all session JSONL files:\n\n```bash\npython3 extract-session-costs.py [--project-slug <slug>] [--output json|yaml]\n```\n\n### Option C: Engram SDK\n\nWhen the Engram SDK is integrated, it tracks costs automatically:\n```typescript\nengram.track({\n  operation: 'create',\n  modelId: 'claude-opus-4-6',\n  inputTokens: 36533,\n  outputTokens: 97287,\n  cacheReadTokens: 130057002,\n  cacheCreationTokens: 3267470,\n  durationMs: 10800000,\n})\n```\n\n## Cost Breakdown Insights\n\nFrom real session data, the typical cost distribution is:\n- **Cache reads: ~70%** of total cost (context re-reading)\n- **Cache writes: ~28%** (new context caching)\n- **Output tokens: ~1.5%** (model responses)\n- **Input tokens: <1%** (most input is cached)\n\nThis means the biggest cost driver is context size, not output length.\nShorter context windows dramatically reduce costs.\n\n## Integration with Engram\n\nSession memories with cost frontmatter are:\n1. Synced to Engram via GitHub Sync (scans `.memory/` repos)\n2. Rendered in the Analytics dashboard (cost charts, model breakdown)\n3. Used for projected cost calculations\n4. Searchable via Full-Text Search\n\nThe nightly CI workflow (`refresh-demo.yml`) picks up new data automatically\nand updates the live demo at https://engram-hq.github.io/releases/.\n",
      "chronoIndex": 5
    },
    {
      "org": "sreniatnoc",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: sreniatnoc-org-knowledge\ndescription: |\n  Knowledge base for sreniatnoc GitHub organization. Use this skill when:\n  - Working on any sreniatnoc repo\n  - Planning features, PRs, or commits\n  - Understanding repo purposes and relationships\n  MANDATORY: Consult before any sreniatnoc repo operation.\nlast_updated: 2026-02-10\n---\n\n# sreniatnoc Organization Knowledge\n\n## Organization Overview\n\n**Mission**: Open-source developer tools in Rust and Go.\n**GitHub**: https://github.com/sreniatnoc\n\n---\n\n## Repos\n\n### rusd (PUBLIC)\n**Purpose**: High-performance embedded database written in Rust with MVCC support. Etcd replacement.\n**Tech**: Rust 2021, gRPC (tonic 0.11/prost 0.12), sled, Docker\n**Repo**: https://github.com/sreniatnoc/rusd\n\n**Key Features**:\n- MVCC (Multi-Version Concurrency Control) storage engine (sled-backed)\n- etcd-compatible gRPC API (KV, Auth, Cluster, Watch, Lease, Maintenance)\n- Watch event delivery with prev_kv support\n- Txn CAS (Compare-and-Swap) operations\n- Raft consensus module (single-node works, multi-node event loop disabled)\n- Docker and Kubernetes support (Docker image 167MB)\n- HTTP/2 keepalive and graceful shutdown (serve_with_shutdown)\n- **Validated as K8s etcd backend** - Kind cluster with K8s v1.35 fully works\n\n**Key Directories**:\n- `src/api/` - 6 gRPC service implementations (kv, watch, lease, cluster, maintenance, auth)\n- `src/storage/` - MVCC storage engine (backend.rs, mvcc.rs, index.rs, compaction.rs)\n- `src/raft/` - Raft consensus (node.rs 677 lines, state.rs, log.rs, transport.rs)\n- `src/watch/` - WatchHub with DashMap + crossbeam channels (wired to KvService and WatchService)\n- `src/server/` - Server orchestration, startup, graceful shutdown\n- `proto/etcdserverpb/` - rpc.proto, kv.proto, auth.proto\n- `benches/` - Criterion benchmarks\n- `scripts/` - k8s-test.sh (492 lines), setup.sh (284 lines)\n- `tests/` - 12 integration tests (BROKEN: test HTTP endpoints but rusd is gRPC-only)\n- `.skills/kind-validation.md` - Kind cluster config and validation details\n\n**Docs**: `README.md`, `STORAGE_DESIGN.md`, `MVCC_IMPLEMENTATION.md`, `FILE_STRUCTURE.md`\n\n**Validated Status (2026-02-10)**:\n- Native build: OK (cargo build --release, ~8,300 lines Rust)\n- Docker build: OK (167MB, requires fixed Dockerfile)\n- Unit tests: 40/40 PASS (storage, mvcc, watch, lease, auth, compaction, raft)\n- Integration tests: 12/12 FAIL (tests use HTTP/JSON but rusd is gRPC-only, needs rewrite)\n- Kind cluster (K8s v1.35): WORKS - all CRUD operations validated\n\n**etcdctl Compatibility Matrix**:\n| API | Status | Notes |\n|-----|--------|-------|\n| KV Put | OK | Works with etcdctl |\n| KV Get | OK | Single key and prefix queries |\n| KV Delete | OK | Returns delete count |\n| Watch | OK | Prefix watch, PUT/DELETE events, prev_kv delivered |\n| Txn CAS | OK | Compare operations (version, create_rev, mod_rev, value) work |\n| Lease Grant | OK | TTL and ID returned |\n| Lease List | OK | Lists active leases |\n| Lease TimeToLive | OK | Returns remaining TTL |\n| Lease Revoke | PARTIAL | Revokes lease but keys NOT deleted (BUG) |\n| Endpoint Health | OK | Reports healthy |\n| Endpoint Status | FAIL | Panics in etcdctl (divide by zero) |\n| Member List | OK | Returns single-node member |\n| User Add | OK | Creates user |\n| Role Add | OK | Creates role |\n| User/Role List | EMPTY | Returns empty lists |\n\n**K8s Operations Validated (Kind, K8s v1.35)**:\n- Namespace create/delete, ConfigMap, Secret, ServiceAccount CRUD\n- Role, RoleBinding CRUD\n- Deployment create, scale (replicas), rolling update\n- Service creation, Pod exec\n- RBAC bootstraps correctly, all control plane components functional\n\n**Known Limitations**:\n- \"Too large resource version\" errors: non-fatal, K8s retries automatically\n- Revision-based range reads: MVCC stores latest only (not full history), sufficient for K8s\n- Integration tests: need rewrite from HTTP/JSON to gRPC (tonic client)\n- Lease revoke doesn't delete attached keys\n- Endpoint status response missing fields (causes etcdctl panic)\n\n**Dockerfile Fixes Applied**:\n- Added `COPY build.rs ./` (protobuf code generation)\n- Added `COPY proto proto/` (proto definitions)\n- Added `COPY benches benches/` and `COPY tests tests/` (Cargo.toml references them)\n- Updated `rust:1.75-slim` -> `rust:1.85-slim` (Cargo.lock v4 format)\n- Fixed `as` -> `AS` casing\n\n---\n\n### k8sify (PUBLIC)\n**Purpose**: Intelligent Docker Compose to Kubernetes migration tool.\n**Tech**: Go\n**Repo**: https://github.com/sreniatnoc/k8sify\n\n**Key Features**:\n- Cost analysis for K8s migration\n- Security scanning\n- Production patterns generation\n- Docker Compose \u2192 K8s manifest conversion\n\n---\n\n### publications (PRIVATE)\n**Purpose**: LinkedIn articles and publication artifacts for all sreniatnoc projects.\n**Repo**: https://github.com/sreniatnoc/publications\n\n**Structure**:\n```\npublications/\n\u251c\u2500\u2500 rusd/\n\u2502   \u2514\u2500\u2500 YYYY-MM-DD-<slug>/\n\u2502       \u251c\u2500\u2500 html/       # LinkedIn-safe HTML\n\u2502       \u251c\u2500\u2500 images/     # PNGs + SVG sources\n\u2502       \u2514\u2500\u2500 post.txt    # Short-form post text\n```\n\n**Rule**: Article artifacts (HTML, images, cover) go here, NOT in project repos. Code/docs/benchmarks stay in project repos.\n\n---\n\n### homebrew-tap (PUBLIC)\n**Purpose**: Homebrew tap for sreniatnoc tools.\n**Repo**: https://github.com/sreniatnoc/homebrew-tap\n\n**Usage**:\n```bash\nbrew tap sreniatnoc/tap\nbrew install k8sify\n```\n",
      "chronoIndex": 5
    },
    {
      "org": "sreniatnoc",
      "repo": "rusd",
      "tier": 3,
      "path": ".skills/kind-validation.md",
      "name": "kind-validation.md",
      "content": "# Kind Cluster Validation for rusd\n\n**Last Updated**: 2026-02-10\n**K8s Version**: v1.35.0 (kind v0.31.0)\n**Status**: WORKING - Full K8s CRUD operations validated\n\n---\n\n## Kind Configuration\n\nFile: `/tmp/kind-rusd.yaml`\n\n```yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnetworking:\n  apiServerPort: 6443\nnodes:\n  - role: control-plane\n    kubeadmConfigPatches:\n      - |\n        kind: ClusterConfiguration\n        etcd:\n          external:\n            endpoints:\n              - http://host.docker.internal:2479\n```\n\nThis config tells Kind to use an external etcd at `http://host.docker.internal:2479` instead of spinning up its own etcd pod. The `host.docker.internal` DNS name resolves to the host machine from inside Docker containers.\n\n---\n\n## Launch Commands\n\n### 1. Start rusd on the host\n\n```bash\ncd /Users/shaileshpant/src/gh-orgs/sreniatnoc/rusd\ncargo run --release -- --listen-client-urls http://0.0.0.0:2479 --advertise-client-urls http://0.0.0.0:2479\n```\n\nOr use the pre-built binary:\n\n```bash\n./rusd --listen-client-urls http://0.0.0.0:2479 --advertise-client-urls http://0.0.0.0:2479\n```\n\n### 2. Create the Kind cluster\n\n```bash\nkind create cluster --name rusd-test --config /tmp/kind-rusd.yaml\n```\n\n### 3. Verify the cluster\n\n```bash\nkubectl cluster-info --context kind-rusd-test\nkubectl get nodes\nkubectl get pods -A\n```\n\n### 4. Teardown\n\n```bash\nkind delete cluster --name rusd-test\n```\n\n---\n\n## etcdctl Compatibility Matrix\n\nAll commands tested with:\n```bash\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=http://localhost:2479\n```\n\n### KV Operations - ALL WORK\n\n| Command | Status | Notes |\n|---------|--------|-------|\n| `etcdctl put foo bar` | OK | Returns \"OK\" |\n| `etcdctl get foo` | OK | Returns key + value |\n| `etcdctl get --prefix foo` | OK | Returns all keys with prefix |\n| `etcdctl del foo` | OK | Returns delete count |\n\n### Watch Operations - ALL WORK\n\n| Command | Status | Notes |\n|---------|--------|-------|\n| `etcdctl watch foo` | OK | Receives PUT events |\n| `etcdctl watch --prefix /` | OK | Receives all events |\n| PUT events | OK | Delivered with key, value, mod_revision |\n| DELETE events | OK | Delivered with key, mod_revision |\n| prev_kv in events | OK | Previous key-value included in watch events |\n\n### Lease Operations - ALL WORK\n\n| Command | Status | Notes |\n|---------|--------|-------|\n| `etcdctl lease grant 300` | OK | Returns lease ID and TTL |\n| `etcdctl lease list` | OK | Lists active lease IDs |\n| `etcdctl lease timetolive <id>` | OK | Returns remaining TTL |\n| `etcdctl lease revoke <id>` | OK | Revokes the lease |\n\n### Cluster Operations\n\n| Command | Status | Notes |\n|---------|--------|-------|\n| `etcdctl member list` | OK | Returns single-node member |\n| `etcdctl endpoint health` | OK | Reports healthy |\n| `etcdctl endpoint status` | FAIL | Panics etcdctl (divide by zero) |\n\n### Auth Operations\n\n| Command | Status | Notes |\n|---------|--------|-------|\n| `etcdctl user add` | OK | Creates user |\n| `etcdctl role add` | OK | Creates role |\n| `etcdctl user list` | EMPTY | Returns empty (known gap) |\n| `etcdctl role list` | EMPTY | Returns empty (known gap) |\n\n### Txn Operations\n\n| Command | Status | Notes |\n|---------|--------|-------|\n| CAS (Compare-and-Swap) | OK | Compare operations work correctly |\n\n---\n\n## Known Limitations\n\n### \"Too large resource version\" errors (NON-FATAL)\n- K8s occasionally logs: `\"Too large resource version\"`\n- These are non-fatal; Kubernetes retries automatically and operations succeed\n- Root cause: revision numbering diverges from what K8s informers expect in some edge cases\n\n### Revision-based range reads (APPROXIMATE)\n- MVCC only stores the latest version of each key (not full history)\n- Range reads with a specific revision return current data, not historical snapshots\n- This is sufficient for K8s operation but not fully etcd-compatible\n\n### Integration tests broken (12/12 FAIL)\n- Tests use HTTP/JSON endpoints (`/v3/kv/put`, `/health`)\n- rusd is a pure gRPC server with no HTTP gateway\n- Tests need rewrite to use tonic gRPC client\n- Port conflict: all tests hardcode port 12379\n\n---\n\n## K8s Operations Validated\n\nAll of the following operations were tested and confirmed working on the Kind cluster with rusd as the backing store:\n\n| Operation | Command | Status |\n|-----------|---------|--------|\n| Create namespace | `kubectl create namespace test-rusd` | OK |\n| Create configmap | `kubectl create configmap test-cm --from-literal=key=value -n test-rusd` | OK |\n| Create secret | `kubectl create secret generic test-secret --from-literal=password=secret -n test-rusd` | OK |\n| Create serviceaccount | `kubectl create serviceaccount test-sa -n test-rusd` | OK |\n| Create role | `kubectl create role test-role --verb=get --resource=pods -n test-rusd` | OK |\n| Create rolebinding | `kubectl create rolebinding test-rb --role=test-role --serviceaccount=test-rusd:test-sa -n test-rusd` | OK |\n| Create deployment | `kubectl create deployment nginx --image=nginx:alpine -n test-rusd` | OK |\n| Scale deployment | `kubectl scale deployment nginx --replicas=3 -n test-rusd` | OK |\n| Rolling update | `kubectl set image deployment/nginx nginx=nginx:latest -n test-rusd` | OK |\n| Create service | `kubectl expose deployment nginx --port=80 -n test-rusd` | OK |\n| Pod exec | `kubectl exec -it <pod> -n test-rusd -- /bin/sh` | OK |\n| Delete namespace | `kubectl delete namespace test-rusd` | OK |\n\nAll K8s CRUD operations work end-to-end with rusd as the etcd backend.\n",
      "chronoIndex": 6
    },
    {
      "org": "ARTIFACTIQ",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: artifactiq-org-knowledge\ndescription: |\n  Knowledge base for ARTIFACTIQ GitHub organization. Use this skill when:\n  - Working on any ARTIFACTIQ repo\n  - Planning features, PRs, or commits within ARTIFACTIQ org\n  - Need to understand repo purposes and relationships\n  - Determining correct repo for a task\n  MANDATORY: Consult before any ARTIFACTIQ repo operation.\n---\n\n# ARTIFACTIQ Organization Knowledge\n\n## Organization Overview\n\n**Mission**: Visual intelligence platform for artifact/object detection.\n**Core Tech**: YOLOv8 with ONNX and CoreML support.\n**Primary Product**: Artifactiq Core - AI-powered visual detection engine.\n\n---\n\n## Repo Selection Guide\n\n| Task | Repo | File/Path |\n|------|------|-----------|\n| Website updates | `releases` | `index.html`, `images/`, `blog/` |\n| Python SDK work | `sdk-python` | `src/artifactiq/` |\n| GPU monitor tool | `mlgpu` | `mlgpu` (main script) |\n| Claude Code integration | `claude-remote-bridge` | `src/` |\n| Binary releases | `releases` | `install.sh`, binaries |\n\n---\n\n## Active Repos\n\n### releases (PRIMARY)\n**Purpose**: Binary distribution hub AND website for Artifactiq.\n**Tech**: Go binaries, HTML/CSS, Bash\n**Key Files**:\n- `index.html` - **ARTIFACTIQ WEBSITE** (112KB+)\n- `install.sh` - Installation script\n- `images/` - 26+ demo images\n- `blog/` - Blog content\n\n**Important**: This is where the actual Artifactiq website lives, NOT in `website` repo.\n\n### sdk-python\n**Purpose**: Official Python SDK for Artifactiq API.\n**Tech**: Python 3.8+, httpx\n**Key Files**:\n- `src/artifactiq/client.py` - Main API client\n- `src/artifactiq/models.py` - Data models\n- `pyproject.toml` - Package config\n\n**API Endpoints**:\n- Artifact detection\n- Product matching\n- Tourism package generation\n\n### mlgpu\n**Purpose**: Apple Silicon GPU monitor for ML training.\n**Tech**: Bash, Python, macOS APIs\n**Key Files**:\n- `mlgpu` - Main executable (20KB+ Bash)\n- `install.sh` - Installation\n- `tools/` - CoreML to ONNX converter\n\n**Supported Frameworks**: Apple Create ML, PyTorch, Ultralytics YOLO, HuggingFace.\n\n### claude-remote-bridge\n**Purpose**: Bidirectional remote Claude Code communication via ntfy.sh.\n**Tech**: Python, requests, ntfy.sh\n**Key Files**:\n- `src/bridge.py` - Core daemon\n- `src/crb` - CLI wrapper\n- `src/claude_integration.py` - Claude hooks\n- `examples/hooks.json` - Hook config\n\n---\n\n## Empty Repos (Placeholders)\n\n| Repo | Intended Use | Priority |\n|------|--------------|----------|\n| `cli` | CLI tools | TBD |\n| `docs` | Documentation site | TBD |\n| `examples` | Code examples | TBD |\n| `openapi` | API specifications | TBD |\n| `sdk-javascript` | JS/Node.js SDK | TBD |\n\n---\n\n## Redundancy Warning\n\n**`website` repo is REDUNDANT**. It contains a Next.js app but the actual production website is the static HTML in `releases/index.html`.\n\nWhen asked to update \"the website\", use `releases` repo, NOT `website`.\n\n---\n\n## Architecture\n\n```\nARTIFACTIQ Platform\n\u251c\u2500\u2500 Detection Engine (Artifactiq Core)\n\u2502   \u2514\u2500\u2500 YOLOv8 ONNX models\n\u251c\u2500\u2500 Distribution (releases repo)\n\u2502   \u251c\u2500\u2500 Binaries (Linux x86_64, macOS arm64)\n\u2502   \u251c\u2500\u2500 Website (index.html)\n\u2502   \u2514\u2500\u2500 Install scripts\n\u251c\u2500\u2500 SDK Layer\n\u2502   \u251c\u2500\u2500 Python SDK (sdk-python) \u2713\n\u2502   \u2514\u2500\u2500 JavaScript SDK (sdk-javascript) - planned\n\u251c\u2500\u2500 Developer Tools\n\u2502   \u251c\u2500\u2500 mlgpu (GPU monitoring)\n\u2502   \u2514\u2500\u2500 claude-remote-bridge (Claude Code integration)\n\u2514\u2500\u2500 Documentation (docs, examples, openapi) - planned\n```\n\n---\n\n## Related: mlOS-foundation\n\nARTIFACTIQ uses **Axon** (from mlOS-foundation) for model installation.\nThe `releases` repo references Axon for downloading YOLOv8 models.\n\nFor Axon/model infrastructure issues, check `mlOS-foundation/axon`.\n",
      "chronoIndex": 4
    },
    {
      "org": "ARTIFACTIQ",
      "repo": ".skills",
      "tier": 2,
      "path": "training/ml-training.md",
      "name": "ml-training.md",
      "content": "---\nname: artifactiq-training-knowledge\ndescription: |\n  Training configuration and status for Artifactiq ML models. Use this skill when:\n  - Resuming training after session failure\n  - Checking training status and configuration\n  - Starting new training runs\n  - Understanding training optimization settings\n  MANDATORY: Read this before any training operation.\nlast_updated: 2026-02-03T13:30:00\n---\n\n# Artifactiq Model Training Knowledge\n\n## Current Best Model: E7 / v5.0.0 (2026-02-03)\n\n| Item | Value |\n|------|-------|\n| **Model** | E7 (released as v5.0.0) |\n| **Release** | https://github.com/ARTIFACTIQ/releases/releases/tag/v5.0.0 |\n| **mAP50** | 0.001454 (4.6x better than E2) |\n| **mAP50-95** | 0.000409 |\n| **Precision** | 0.0022 |\n| **Recall** | 0.042 |\n| **Config** | 46 shards \u00d7 3 epochs, batch=8, freeze=10 |\n| **Training Time** | ~3.5 hours |\n\n### Known Issue: Low Confidence Scores\n\nE7 produces very low confidence scores (1-3% instead of 50-90%) due to `val=False` during training, which disabled confidence calibration. **Use E8 calibrated training for future models.**\n\n### Model Files\n```\n/Users/shaileshpant/src/orgs/artifactiq/ml-models/training/runs/production/\n\u251c\u2500\u2500 e7_best.pt      (40.5 MB) - PyTorch\n\u2514\u2500\u2500 e7_best.onnx    (76.8 MB) - ONNX\n```\n\n### Full Model Path\n```\n/Users/shaileshpant/src/orgs/artifactiq/ml-models/training/runs/e7_44shard/shard_45/weights/best.pt\n```\n\n---\n\n## Model Comparison\n\n| Model | mAP50 | Training | Status |\n|-------|-------|----------|--------|\n| E2 | 0.000316 | 22 shards \u00d7 1 epoch | Previous best |\n| E6 | 0.000170 | 46 shards \u00d7 1 epoch | \u274c Regressed |\n| **E7** | **0.001454** | **46 shards \u00d7 3 epochs** | \u2705 **Current best** |\n\n**Key Learning:** 3 epochs per shard is critical. 1 epoch causes regression.\n\n---\n\n## Notifications\n\n| Topic | URL |\n|-------|-----|\n| artifactiq-ntfy | `ntfy.sh/artifactiq-ntfy` |\n\n---\n\n## Training Configuration (Validated)\n\n### E8 Calibrated Settings (RECOMMENDED BASELINE)\n\nUse `e8_calibrated_train.py` for proper confidence calibration:\n\n```python\n{\n    # Shard config\n    \"shards\": 46,\n    \"images_per_shard\": 1500,\n    \"epochs_per_shard\": 10,     # Full mode (5 for test)\n\n    # Speed optimizations\n    \"batch\": 8,\n    \"mini_val\": 100,            # 100 images \u2248 6.7% of shard\n\n    # CRITICAL: Confidence Calibration\n    \"val\": True,                # MUST BE TRUE for calibration\n    \"patience\": 5,              # Early stopping\n\n    # Augmentation - DISABLED for stability when fine-tuning E7\n    # E7 has uncalibrated weights, augmentation causes gradient explosion\n    \"mosaic\": 0.0,              # Disabled (0.5 caused instability)\n    \"mixup\": 0.0,               # Disabled (0.1 caused instability)\n    \"copy_paste\": 0.0,\n\n    # Learning Rate - CONSERVATIVE for E7 fine-tuning\n    \"lr0\": 0.001,               # Same as E7 (0.005 caused gradient explosion)\n    \"lrf\": 0.01,\n    \"warmup_epochs\": 1,\n    \"optimizer\": \"AdamW\",\n\n    # Layer freezing (less aggressive)\n    \"freeze\": 5,                # Was 10 in E7\n    \"cos_lr\": True,\n\n    # MPS Compatibility\n    \"workers\": 0,\n    \"cache\": False,\n    \"rect\": False,\n\n    # Model chaining\n    \"use_best_pt\": True,        # Chain with best.pt not last.pt\n}\n```\n\n### Legacy E7 Settings (DO NOT USE - causes low confidence)\n```python\n{\n    \"val\": False,       # \u274c WRONG - disables confidence calibration\n    \"patience\": 0,      # \u274c No early stopping\n    \"mosaic\": 0.0,      # \u274c No augmentation\n    \"mixup\": 0.0,\n    \"freeze\": 10,       # Too aggressive\n    \"lr0\": 0.001,       # Too low\n}\n```\n\n---\n\n## Quick Commands\n\n### Start Training (E8+ Calibrated)\n```bash\ncd /Users/shaileshpant/src/orgs/artifactiq/ml-models/training\nsource venv/bin/activate\n\n# Test run (2 shards, 5 epochs each)\npython e8_calibrated_train.py\n\n# Full run (all shards, 10 epochs each)\nnohup python -u e8_calibrated_train.py --full > e8_training.log 2>&1 &\n\n# Custom epochs\npython e8_calibrated_train.py --full --epochs 15\n```\n\n### Monitor\n```bash\ntail -f /Users/shaileshpant/src/orgs/artifactiq/ml-models/training/e8_training.log\ngrep -E \"(Completed|mAP50)\" e8_training.log\n\n# Get accurate GPU memory (from training log, not ps aux)\ngrep -E \"[0-9]+\\.[0-9]+G\" e8_training.log | tail -1\n\n# Check completed shards\ngrep -c \"SUCCESS: Shard\" e8_training.log\n```\n\n### Resume After OOM/Crash\n```bash\n# Find last completed shard\nLAST=$(grep -c \"SUCCESS: Shard\" e8_training.log)\necho \"Resume from shard $LAST\"\n\n# Resume training\nnohup python -u e8_calibrated_train.py --full --start-shard $LAST > e8_training.log 2>&1 &\n```\n\n### Validate Model\n```bash\ncd /Users/shaileshpant/src/orgs/artifactiq/ml-models/training\nsource venv/bin/activate\npython -c \"\nfrom ultralytics import YOLO\nmodel = YOLO('runs/production/e7_best.pt')\nresults = model.val(data='/path/to/dataset.yaml', batch=8, device='mps')\nprint(f'mAP50: {results.box.map50:.6f}')\n\"\n```\n\n---\n\n## Dataset Info\n\n| Item | Value |\n|------|-------|\n| Location | `/Users/shaileshpant/src/orgs/artifactiq-models/runs/merged_all_datasets/` |\n| Training Images | 67,991 |\n| Validation Images | 7,570 |\n| Classes | 39 |\n\n---\n\n## Memory Monitoring on Apple Silicon\n\n### Key Learnings\n\n**DO NOT use `ps aux` for memory monitoring** - it reports misleading values due to:\n1. Shared libraries counted multiple times across processes\n2. MPS (Metal) pre-allocates system RAM as GPU memory\n3. Memory that's freed but not yet reclaimed by OS\n\n### Accurate Memory Checks\n\n```bash\n# GPU memory (from training log) - MOST ACCURATE\ngrep -E \"[0-9]+\\.[0-9]+G\" e8_training.log | tail -1\n# Expected: ~5.4G stable per shard\n\n# System RAM overview\nvm_stat | head -5\ntop -l 1 -n 0 | grep PhysMem\n\n# MPS allocated memory (needs ioreg parsing)\nioreg -l | grep \"In use system memory\" | head -1\n```\n\n### Memory Architecture\n\n| Component | Typical Value | Notes |\n|-----------|---------------|-------|\n| GPU (MPS) | 5-6 GB | Stable across shards |\n| Subprocess RAM | 2-3 GB | Releases on shard completion |\n| Main orchestrator | ~250 MB | Minimal footprint |\n| MPS pre-allocation | ~10 GB | Normal for Metal on Apple Silicon |\n\n### Subprocess Memory Isolation\n\nTraining uses `subprocess.run()` per shard - memory IS reclaimed when subprocess exits. Each shard runs in complete isolation with:\n- Fresh Python interpreter\n- `gc.collect()` + `torch.mps.empty_cache()` at start\n- Full cleanup on exit\n\n**No memory leak risk** - if `ps aux` shows high values, it's misleading shared memory accounting.\n\n---\n\n## Root Cause: E7 Low Confidence Scores\n\nE7 model produces 1-3% confidence instead of typical 50-90%. Root cause analysis:\n\n| Setting | E7 (Bad) | E8 (Fixed) | Impact |\n|---------|----------|------------|--------|\n| `val` | `False` | `True` | **Critical** - No confidence calibration |\n| `mosaic` | `0.0` | `0.5` | No augmentation \u2192 overfitting |\n| `freeze` | `10` | `5` | Detection head can't adapt |\n| `lr0` | `0.001` | `0.005` | Learning too slow |\n| `patience` | `0` | `5` | No early stopping |\n\n**Solution:** Use `e8_calibrated_train.py` with proper settings.\n\n---\n\n## Changelog\n\n| Date | Change |\n|------|--------|\n| 2026-02-03 | **Memory monitoring guide** - Apple Silicon memory accounting learnings |\n| 2026-02-03 | **E8 stable config** - Disabled mosaic/mixup, lr0=0.001 for stability |\n| 2026-02-03 | **E8 calibrated training** - Added proper val/augmentation/lr |\n| 2026-02-03 | Root cause analysis: val=False disables confidence calibration |\n| 2026-02-03 | **E7 RELEASED** - mAP50: 0.001454 (but low confidence) |\n| 2026-02-03 | E7 training complete (46 shards \u00d7 3 epochs) |\n| 2026-02-02 | E6 regressed (mAP50: 0.000170) - 1 epoch insufficient |\n| 2026-02-02 | Optimized training: batch=8, mini_val=100, freeze=10 |\n| 2026-01-31 | E2 baseline (mAP50: 0.000316) |\n",
      "chronoIndex": 5
    },
    {
      "org": "ARTIFACTIQ",
      "repo": ".skills",
      "tier": 2,
      "path": "training/shard-training.md",
      "name": "shard-training.md",
      "content": "---\nname: shard-training\ndescription: |\n  Run shard-based calibrated training for Artifactiq YOLO models.\n  MANDATORY: Read this before starting any training run.\nlast_updated: 2026-02-06\n---\n\n# Shard-Based Calibrated Training\n\n## Script\n```\n/Users/shaileshpant/src/orgs/artifactiq/ml-models/training/e8_calibrated_train.py\n```\n\n## Usage\n\n```bash\ncd /Users/shaileshpant/src/orgs/artifactiq/ml-models/training\nsource venv/bin/activate\n\n# Full run (46 shards x 10 epochs) with specific base model\nnohup python -u e8_calibrated_train.py --full \\\n  --base-model runs/production/e8_1_wavg_best.pt \\\n  --run-name e9_calibrated \\\n  > e9_training.log 2>&1 &\n\n# Test run (2 shards x 5 epochs) - for quick validation\npython e8_calibrated_train.py \\\n  --base-model runs/production/current_best.pt\n\n# Resume from shard N\nnohup python -u e8_calibrated_train.py --full \\\n  --base-model runs/production/current_best.pt \\\n  --run-name e9_calibrated \\\n  --start-shard 20 \\\n  > e9_training.log 2>&1 &\n\n# Custom epochs\npython e8_calibrated_train.py --full --epochs 15 \\\n  --base-model runs/production/current_best.pt \\\n  --run-name e9_15ep\n```\n\n## CLI Flags\n\n| Flag | Default | Description |\n|------|---------|-------------|\n| `--full` | off | Run all 46 shards (otherwise 2 test shards) |\n| `--base-model` | `runs/production/current_best.pt` | Starting checkpoint |\n| `--run-name` | auto from model stem | Output dir name under `runs/` |\n| `--start-shard` | 0 | Resume from shard N |\n| `--epochs` | 10 (full) / 5 (test) | Epochs per shard |\n\n## Training Config (Calibrated Defaults)\n\n| Parameter | Value | Reason |\n|-----------|-------|--------|\n| batch | 8 | MPS memory safe |\n| device | mps | Apple Metal |\n| workers | 0 | MPS compatibility |\n| cache | False | 68K images too large |\n| optimizer | AdamW | Stable adaptive |\n| lr0 | 0.001 | Conservative (0.005 caused gradient explosion) |\n| lrf | 0.01 | Final LR multiplier |\n| warmup_epochs | 1 | Short warmup |\n| freeze | 5 | Less aggressive than E7's 10 |\n| cos_lr | True | Cosine annealing |\n| val | True | **CRITICAL** for confidence calibration |\n| patience | 5 | Early stopping |\n| mosaic | 0.0 | Disabled for stability |\n| mixup | 0.0 | Disabled for stability |\n\n## Dataset\n\n| Item | Value |\n|------|-------|\n| Path | `/Users/shaileshpant/src/orgs/artifactiq-models/runs/merged_all_datasets/` |\n| Train | 67,991 images |\n| Val | 7,570 images |\n| Classes | 39 |\n| Shards | 46 x 1500 images |\n| Mini-val | 100 images (sampled for fast per-shard validation) |\n\n## Production Models\n\n| Model | Path | Metrics |\n|-------|------|---------|\n| E7 (v5.0.0) | `runs/production/e7_best.pt` | mAP50:0.001454, low confidence |\n| E8 (shard_26) | `runs/production/e8_best.pt` | P:0.407, R:0.235, F1:0.298 |\n| E8.1 (wavg) | `runs/production/e8_1_wavg_best.pt` | P:0.382, R:0.261, F1:0.311 |\n| E8.2 | `runs/production/e8_2_best.pt` | Same as E8.1 + GT audit release |\n| current | `runs/production/current_best.pt` | Symlink to latest best |\n\n## How It Works\n\n1. **Sharding**: Dataset split into 46 shards of 1500 images each\n2. **Sequential training**: Each shard trains for N epochs\n3. **best.pt chaining**: After each shard, the best checkpoint (not last) feeds into next shard\n4. **Subprocess isolation**: Each shard runs in a fresh Python process (prevents MPS memory leaks)\n5. **Mini-val**: 100-image validation subset for fast per-shard metrics\n6. **ntfy notifications**: Progress updates sent to `artifactiq-ntfy`\n\n## Monitoring\n\n```bash\n# Watch log\ntail -f e9_training.log\n\n# Check completed shards\ngrep \"SUCCESS: Shard\" e9_training.log | wc -l\n\n# Check for failures\ngrep \"FAILED\" e9_training.log\n\n# Check metrics\ngrep -E \"(mAP50|Completed)\" e9_training.log\n```\n\n## After Training\n\n1. Find the final model: last successful shard's `best.pt`\n2. Copy to production: `cp runs/e9_calibrated/shard_45/weights/best.pt runs/production/e9_best.pt`\n3. Update symlink: `ln -sf e9_best.pt runs/production/current_best.pt`\n4. Run weighted avg retry if shards failed (see `e8_retry_failed_shards.py`)\n5. Export ONNX: `python export_v4.py runs/production/e9_best.pt`\n\n## Key Learnings\n\n- **val=True is critical** - E7 had val=False which broke confidence calibration\n- **3+ epochs per shard** - 1 epoch causes regression (E6 lesson)\n- **lr0=0.001** - 0.005 caused gradient explosion with uncalibrated E7 weights\n- **mosaic=0.0** - mosaic augmentation caused instability during fine-tuning\n- **best.pt chaining** - prevents quality degradation from overfitted last.pt\n- **Weighted avg retry** - prevents catastrophic forgetting when retrying failed shards\n- **Don't create per-epoch scripts** - use --base-model and --run-name flags instead\n- **Some shards early-stop with ~0 metrics** - data subset quality varies; expect 2-5 bad shards per run\n- **Mini-val (100 images) is noisy** - individual shard metrics don't reflect true model quality; judge after full run\n- **current_best.pt symlink** - useful default; update after each successful training round\n- **Repeat-offender shards:** Shards 5 and 14 fail consistently but NOT due to bad data (gt-audit confirmed)\n- **Most \"failed\" shards are transient** - shards 0,1,7,8 failed E8 but passed E9 fine\n- **Shard failures = distribution mismatch + chaining state** - not label quality. Weighted avg retry is the correct fix\n- **gt-audit at low confidence produces misleading issue counts** - use conf >= 0.25 for meaningful comparison\n\n## Training History\n\n| Run | Base | Shards | Epochs | Result |\n|-----|------|--------|--------|--------|\n| E8 | E7 best.pt | 46 | 10 | P:0.407 R:0.235 F1:0.298 |\n| E8.1 (wavg retry) | E8 shard_26 | failed shards | 5 | P:0.382 R:0.261 F1:0.311 |\n| E9 (in progress) | E8.1 wavg | 46 | 10 | 7/46 done, best shard mAP50:0.065 |\n",
      "chronoIndex": 6
    },
    {
      "org": "ARTIFACTIQ",
      "repo": ".skills",
      "tier": 2,
      "path": "training/e10-wavg-anchoring.md",
      "name": "e10-wavg-anchoring.md",
      "content": "# Skill: E10 Wavg Anchoring Training\n\n## Overview\nE10 training uses periodic weighted average anchoring as the DEFAULT strategy to prevent catastrophic forgetting. Instead of pure sequential shard chaining (which failed in E9 at shard 30), the model is blended back to an anchor every N shards.\n\n## Quick Reference\n```\nAnchor interval (N):     5 shards\nAlpha (blend strength):  0.80 (anchor portion)\nBeta (anchor update):    0.90 (anchor absorbs 10% of accepted blend)\nHoldout size:            200 images (fixed seed)\nQuality gate:            mAP50 regression tolerance = 0.01\nHalt condition:          2 consecutive rejected cycles\nShard config:            Same as E8/E9 (10 epochs, lr0=0.001, freeze=5, patience=5)\nExpected cycles:         9 blend cycles for 46 shards\n```\n\n## Key Concept\n```\nanchor = base_model\nfor each cycle (5 shards):\n  train 5 shards sequentially (chain best.pt)\n  blended = 0.80 * anchor + 0.20 * chain_result\n  if blended passes quality gate:\n    accept, update anchor = 0.90 * anchor + 0.10 * blended\n  else:\n    reject, rollback to anchor\n```\n\n## Pre-requisites\n- Best E9 shard checkpoint identified (or E8.1 base)\n- Full strategy doc: `ml-models/training/docs/E10_WAVG_ANCHORING_STRATEGY.md`\n- Script modifications to `e8_calibrated_train.py` (new flags: --anchor-interval, --alpha, --anchor-beta)\n\n## Launch Command (when script is ready)\n```bash\ncd /Users/shaileshpant/src/orgs/artifactiq/ml-models/training\nnohup python3 e8_calibrated_train.py \\\n  --full \\\n  --base-model runs/production/e9_best_shard.pt \\\n  --run-name e10_anchored \\\n  --anchor-interval 5 \\\n  --alpha 0.80 \\\n  --anchor-beta 0.90 \\\n  > e10_training.log 2>&1 &\necho \"PID: $!\"\n```\n\n## Monitoring\n```bash\n# Check progress\ntail -20 e10_training.log\n# Check cycle reports\nls runs/e10_anchored/cycle_*/cycle_*_metrics.json\n# Check anchor lineage\ncat runs/e10_anchored/e10_anchor_lineage.json\n```\n\n## Why This Works\n- Bounds maximum weight drift to ~25% of single cycle drift (vs unbounded in E9)\n- E9 forgetting started at shard 30; with N=5, that's 6 anchor resets before reaching shard 30\n- Quality gate prevents degraded blends from propagating\n- Anchor updates let the model improve while staying grounded\n\n## History\n| Run | Strategy | Result |\n|-----|----------|--------|\n| E8 | Sequential 46 shards, no anchoring | 17/46 failed, wavg retry saved it |\n| E9 | Sequential 46 shards, no anchoring | Catastrophic forgetting at shard 30 |\n| E10 | Periodic wavg anchoring every 5 shards | TBD |\n",
      "chronoIndex": 7
    },
    {
      "org": "mlOS-foundation",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: mlos-org-knowledge\ndescription: |\n  Knowledge base for mlOS-foundation GitHub organization. Use this skill when:\n  - Working on any mlOS-foundation repo\n  - Planning features, PRs, or commits within mlOS-foundation org\n  - Need to understand repo purposes and relationships\n  - Determining correct repo for a task\n  MANDATORY: Consult before any mlOS-foundation repo operation.\n---\n\n# mlOS-foundation Organization Knowledge\n\n## Organization Overview\n\n**Mission**: Building the universal operating system for machine learning.\n**Core Innovation**: SMI (Standard Model Interface) - universal contract for ML frameworks.\n**Patent**: US-63/865,176 - Kernel-Level Optimizations for ML Workloads.\n\n---\n\n## Repo Selection Guide\n\n| Task | Repo | File/Path |\n|------|------|-----------|\n| Website updates | `mlosfoundation.org` | `*.html`, `src/` |\n| Model installer (Axon) | `axon` | `cmd/axon/`, `internal/` |\n| Core runtime releases | `core-releases` | `docs/`, `release-notes/` |\n| E2E testing | `system-test` | `scripts/`, `config/` |\n| Libtorch builds | `libtorch-builds` | releases |\n| Ubuntu distro | `mlos-linux-ubuntu` | `roadmap/` |\n| Flatcar distro | `mlos-linux-flatcar` | `roadmap/` |\n| Kernel patches | `mlos-linux-kernel` | `patches/` |\n| Org profile | `.github` | `profile/README.md` |\n\n---\n\n## Active Repos\n\n### axon (CORE TOOL)\n**Purpose**: Universal ML model installer and package manager.\n**Tech**: Go 1.21+\n**Version**: v3.1.9\n**License**: AGPL-3.0\n\n**Key Commands**:\n```bash\naxon install <model>    # Install from any registry\naxon register <model>   # Register with MLOS Core\naxon search <query>     # Discover models\n```\n\n**Key Directories**:\n- `cmd/axon/` - CLI entry point\n- `internal/cache/` - Local cache\n- `internal/registry/` - Registry client\n- `internal/converter/` - Multi-framework conversion\n- `docker/` - ONNX conversion images\n\n**Supported Registries**: Hugging Face Hub, PyTorch Hub, TensorFlow Hub, ModelScope.\n\n### core-releases\n**Purpose**: Public distribution of MLOS Core binaries.\n**Version**: v6.2.0-alpha\n**Note**: Mirrors private `mlOS-foundation/core` repo.\n\n**Key Files**:\n- `docs/` - Release documentation\n- `release-notes/` - Version history\n\n### mlosfoundation.org (WEBSITE)\n**Purpose**: Official mlOS Foundation website.\n**Tech**: HTML/JS/CSS, Netlify/Vercel\n**Domain**: mlosfoundation.org\n\n**Key Files**:\n- `netlify/functions/` - Serverless functions\n- `api/` - Vercel API routes\n- `architecture.html` - Architecture docs\n- `netlify.toml` / `vercel.json` - Deploy config\n\n### system-test\n**Purpose**: E2E testing framework for Axon + Core.\n**Tech**: Bash, Python 3.8+\n\n**Key Files**:\n- `scripts/test-release-e2e.sh.bash` - Main test runner\n- `scripts/generate-metrics.py` - Metrics generator\n- `config/models.yaml` - Model definitions\n- `report/` - HTML report generation\n\n**Tested Model Types**: NLP, Vision, Multimodal, LLM.\n\n### libtorch-builds\n**Purpose**: Pre-built libtorch for ARM64/aarch64 Linux.\n**License**: BSD-3-Clause\n**Latest**: v2.5.1\n\n**Usage**:\n```bash\ncurl -LO https://github.com/mlOS-foundation/libtorch-builds/releases/download/v2.5.1/libtorch-linux-aarch64-2.5.1.tar.gz\n```\n\n### mlos-linux-ubuntu\n**Purpose**: Ubuntu 22.04/24.04 LTS-based ML distribution.\n**Status**: Planning phase\n**Target**: Q2-Q3 2026, v1.0.0\n\n**Key Features**:\n- ML-aware kernel scheduler\n- Tensor memory management\n- GPU resource orchestration\n- Pre-installed MLOS Core + Axon\n\n### mlos-linux-flatcar\n**Purpose**: Flatcar (CoreOS successor) based ML distribution.\n**Status**: Planning phase\n**Target**: Q2-Q3 2026, v1.0.0\n\n**Key Features**:\n- Container-first architecture\n- Immutable filesystem (OSTree)\n- Ignition configuration\n- Containerized MLOS stack\n\n### mlos-linux-kernel\n**Purpose**: Shared kernel patches for MLOS distributions.\n**Status**: Planning phase\n**License**: Proprietary\n\n**Patches**:\n- `mlos-scheduler.patch` - ML-aware scheduler\n- `tensor-memory.patch` - Tensor memory management\n- `gpu-orchestration.patch` - GPU coordination\n\n### .github\n**Purpose**: Organization profile and documentation.\n**Key File**: `profile/README.md` - Org overview displayed on GitHub.\n\n---\n\n## Empty Repos (Placeholders)\n\n| Repo | Intended Use | Priority |\n|------|--------------|----------|\n| `bindings` | Language bindings | TBD |\n| `examples` | Usage examples | TBD |\n| `smi-spec` | SMI specification | High (core to mission) |\n\n---\n\n## Architecture\n\n```\nApplication Layer\n\u251c\u2500\u2500 Axon CLI\n\u251c\u2500\u2500 MLOS API\n\u2514\u2500\u2500 Plugins\n    \u2193\nMLOS Core Engine (core-releases)\n\u251c\u2500\u2500 Model Registry\n\u251c\u2500\u2500 Plugin Registry\n\u251c\u2500\u2500 Resource Management\n\u2514\u2500\u2500 SMI (Standard Model Interface)\n    \u2193\nRuntime Plugins\n\u251c\u2500\u2500 ONNX Runtime\n\u2514\u2500\u2500 GGUF/llama.cpp (LLMs)\n    \u2193\nKernel Layer (mlos-linux-kernel)\n\u251c\u2500\u2500 ML-aware Scheduler\n\u251c\u2500\u2500 Tensor Memory Management\n\u2514\u2500\u2500 GPU Orchestration\n    \u2193\nLinux Distributions\n\u251c\u2500\u2500 mlos-linux-ubuntu\n\u2514\u2500\u2500 mlos-linux-flatcar\n```\n\n---\n\n## Version Compatibility\n\n| Component | Current Version |\n|-----------|-----------------|\n| Axon | v3.1.9 |\n| Core | v6.2.0-alpha |\n| Kernel | v6.2.0-alpha |\n| Ubuntu Distro | Pre-release |\n| Flatcar Distro | Pre-release |\n| libtorch | v2.5.1 |\n\n---\n\n## Related: ARTIFACTIQ\n\nARTIFACTIQ uses Axon for model installation in their `releases` repo.\nFor ARTIFACTIQ-specific issues, check the `ARTIFACTIQ` organization.\n",
      "chronoIndex": 1
    },
    {
      "org": "traceloop-ai",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: traceloop-ai-org-knowledge\ndescription: |\n  Knowledge base for traceloop-ai GitHub organization. Use this skill when:\n  - Working on any traceloop-ai repo\n  - Planning features, PRs, or commits\n  - Understanding repo purposes and relationships\n  MANDATORY: Consult before any traceloop-ai repo operation.\nlast_updated: 2026-02-09\n---\n\n# traceloop-ai Organization Knowledge\n\n## Organization Overview\n\n**Mission**: Production-grade local-first observability for AI agents.\n**GitHub**: https://github.com/traceloop-ai\n\n---\n\n## Repos\n\n### traceloop (PUBLIC)\n**Purpose**: Local-first observability platform for AI agents - trace collection, visualization, and performance analytics.\n**Tech**: Go (server), Python SDK, TypeScript SDK, React (dashboard), BadgerDB\n**Repo**: https://github.com/traceloop-ai/traceloop\n\n**Key Features**:\n- Trace API (gRPC/HTTP) with local BadgerDB storage\n- Web dashboard for real-time trace visualization\n- Multi-framework: LangChain, CrewAI, OpenAI, Anthropic\n- Python & TypeScript SDKs with decorator-based instrumentation\n- Privacy-first: all data stays local\n\n**Key Directories**:\n- `cmd/traceloop/` - CLI entry point\n- `server/` - gRPC service, REST API, storage layer\n- `sdk/python/` - Python SDK\n- `sdk/typescript/` - TypeScript SDK\n- `web/dashboard/` - React dashboard\n- `examples/` - Integration examples\n\n---\n\n### traceloop-website (PUBLIC)\n**Purpose**: Official marketing and documentation website.\n**Tech**: HTML/CSS/JavaScript, GitHub Pages\n**Repo**: https://github.com/traceloop-ai/traceloop-website\n\n**Key Features**:\n- Multi-domain: traceloop-ai.dev, .io, .ai\n- Status dashboard\n- Responsive design\n\n---\n\n### homebrew-tap (PUBLIC)\n**Purpose**: Homebrew installation package for Traceloop CLI.\n**Tech**: Ruby (Homebrew formula)\n**Repo**: https://github.com/traceloop-ai/homebrew-tap\n\n**Usage**:\n```bash\nbrew tap traceloop-ai/tap\nbrew install traceloop\n```\n",
      "chronoIndex": 1
    },
    {
      "org": "reccaller-ai",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: reccaller-ai-org-knowledge\ndescription: |\n  Knowledge base for reccaller-ai GitHub organization. Use this skill when:\n  - Working on any reccaller-ai repo\n  - Planning features, PRs, or commits\n  - Understanding repo purposes and relationships\n  MANDATORY: Consult before any reccaller-ai repo operation.\nlast_updated: 2026-02-09\n---\n\n# reccaller-ai Organization Knowledge\n\n## Organization Overview\n\n**Mission**: RecCall - Stop repeating yourself to AI. Context shortcuts and recipe management for AI IDEs.\n**GitHub**: https://github.com/reccaller-ai\n\n---\n\n## Repos\n\n### reccall (PUBLIC)\n**Purpose**: Core RecCall engine - context shortcuts management system for AI IDEs (Cursor, VSCode, Warp, CLI).\n**Tech**: TypeScript/Node.js, MCP (Model Context Protocol), npm (v2.1.4)\n**Repo**: https://github.com/reccaller-ai/reccall\n\n**Key Features**:\n- CLI tool + MCP server\n- Integrations: Cursor, VSCode, Warp, CLI\n- Context/recipe shortcuts (dynamic, hybrid, static types)\n- Plugin architecture with dependency injection (tsyringe)\n\n**Key Directories**:\n- `src/` - Engine core and SDK\n- `starter-pack/` - Built-in recipes\n- `vscode-extension/` - VSCode integration\n- `warp-integration/` - Warp terminal integration\n\n---\n\n### websites (PUBLIC)\n**Purpose**: Static marketing website for reccaller.ai.\n**Tech**: HTML/CSS/JS, GitHub Pages\n**Repo**: https://github.com/reccaller-ai/websites\n\n**Key Directories**:\n- `pages/` - Getting started, how it works, integrations\n- `assets/` - CSS/JS\n\n---\n\n### .github (PUBLIC)\n**Purpose**: GitHub Actions workflows and org profile.\n**Repo**: https://github.com/reccaller-ai/.github\n\n**Key Directories**:\n- `profile/` - Org branding/README\n- `.github/workflows/` - CI/CD\n\n---\n\n### reccaller-ai (PUBLIC)\n**Purpose**: Organization profile repo (placeholder).\n**Repo**: https://github.com/reccaller-ai/reccaller-ai\n\n---\n\n### roadmaps (PRIVATE)\n**Purpose**: Strategic planning and product roadmaps.\n**Repo**: https://github.com/reccaller-ai/roadmaps\n\n**Key Files**:\n- `RECCALL_ML_PIVOT_PLAN.md` - Comprehensive ML pivot strategy\n\n---\n\n### contexts (PUBLIC)\n**Purpose**: Official context library - version-controlled recipes and context configs for AI development.\n**Tech**: JSON\n**Repo**: https://github.com/reccaller-ai/contexts\n\n**Key Features**:\n- Recipe categories: git, development, deployment, debugging, testing, security, devops\n- Served at https://contexts.reccaller.ai\n- Manifest and index files for discovery\n",
      "chronoIndex": 1
    },
    {
      "org": "prepguides",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: prepguides-org-knowledge\ndescription: |\n  Knowledge base for prepguides GitHub organization. Use this skill when:\n  - Working on any prepguides repo\n  - Planning features, PRs, or commits\n  - Understanding repo purposes and relationships\n  MANDATORY: Consult before any prepguides repo operation.\nlast_updated: 2026-02-09\n---\n\n# prepguides Organization Knowledge\n\n## Organization Overview\n\n**Mission**: Master technical interviews with interactive guides, diagrams, and project collections.\n**GitHub**: https://github.com/prepguides\n\n---\n\n## Repos\n\n### prepguides.dev (PUBLIC)\n**Purpose**: Main website - static HTML site for technical interview preparation with GitHub-integrated content submission.\n**Tech**: JavaScript/Node.js, Vercel\n**Repo**: https://github.com/prepguides/prepguides.dev\n\n**Key Directories**:\n- `auth/` - Authentication\n- `api/` - API endpoints\n- `kubernetes/`, `algorithms/`, `networking/` - Topic areas\n- `docs/` - Documentation\n\n---\n\n### space-sysdesign (PRIVATE)\n**Purpose**: System design catalog - API-driven image crawler system documentation with depth-bounded architecture.\n**Tech**: Markdown/Documentation\n**Repo**: https://github.com/prepguides/space-sysdesign\n\n**Key Features**:\n- Web crawler design patterns\n- API endpoint specifications\n- Distributed architecture examples\n\n---\n\n### go-interviews (PUBLIC)\n**Purpose**: Comprehensive Go project collection for technical interviews - Kubernetes operators and Go patterns.\n**Tech**: Go\n**Repo**: https://github.com/prepguides/go-interviews\n\n**Key Directories**:\n- `operator/` - Kubernetes CRDs, controllers, RBAC\n- `patterns/` - Design patterns, concurrency, testing\n\n---\n\n### diagrams (PUBLIC)\n**Purpose**: Curated visual diagrams for technical interview concepts.\n**Tech**: SVG/Markdown\n**Repo**: https://github.com/prepguides/diagrams\n\n**Key Directories**:\n- `kubernetes/`, `networking/`, `databases/`, `microservices/`, `system-design/`\n\n---\n\n### spring-boot-interview-scaffold (PUBLIC)\n**Purpose**: Spring Boot scaffolding template for coding interviews.\n**Tech**: Java (Spring Boot 3.2, Java 17), Maven, H2, JPA\n**Repo**: https://github.com/prepguides/spring-boot-interview-scaffold\n\n---\n\n### space-dsalgo (PRIVATE)\n**Purpose**: Advanced coding problems for senior engineer interview prep.\n**Tech**: Java\n**Repo**: https://github.com/prepguides/space-dsalgo\n\n**Key Features**:\n- 100+ test cases\n- Distributed systems, stream processing, thread-safe implementations\n- Problems: job intervals, Dijkstra, closest ancestor, booking systems\n",
      "chronoIndex": 1
    },
    {
      "org": "go-foundations",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: go-foundations-org-knowledge\ndescription: |\n  Knowledge base for go-foundations GitHub organization. Use this skill when:\n  - Working on any go-foundations repo\n  - Planning features, PRs, or commits\n  - Understanding repo purposes and relationships\n  MANDATORY: Consult before any go-foundations repo operation.\nlast_updated: 2026-02-09\n---\n\n# go-foundations Organization Knowledge\n\n## Organization Overview\n\n**Mission**: Foundation Go libraries - high-performance, production-grade Go packages.\n**GitHub**: https://github.com/go-foundations\n\n---\n\n## Repos\n\n### workerpool (PUBLIC)\n**Purpose**: High-performance generic worker pool library for concurrent job processing with multiple distribution strategies.\n**Tech**: Go (generics)\n**Repo**: https://github.com/go-foundations/workerpool\n\n**Key Features**:\n- Type-safe using Go generics\n- 5 distribution strategies: Round-Robin, Chunked, Work Stealing, Priority-Based, Adaptive\n- Built-in metrics and monitoring\n- Thread-safe with context support\n- Chase-Lev work stealing deque, binary heap priority queue\n\n**Key Directories**:\n- `strategies/` - Distribution strategy implementations\n- `examples/` - Usage examples (custom strategy, HTTP, string processing)\n- `benchmarks/` - Performance benchmarking suite\n- `docs/` - Documentation\n",
      "chronoIndex": 1
    },
    {
      "org": "fedai-oss",
      "repo": ".skills",
      "tier": 2,
      "path": "org-knowledge/SKILL.md",
      "name": "SKILL.md",
      "content": "---\nname: fedai-oss-org-knowledge\ndescription: |\n  Knowledge base for fedai-oss GitHub organization. Use this skill when:\n  - Working on any fedai-oss repo\n  - Planning features, PRs, or commits\n  - Understanding repo purposes and relationships\n  MANDATORY: Consult before any fedai-oss repo operation.\nlast_updated: 2026-02-09\n---\n\n# fedai-oss Organization Knowledge\n\n## Organization Overview\n\n**Mission**: Federated AI - Open-source federated learning framework in Go.\n**GitHub**: https://github.com/fedai-oss\n\n---\n\n## Repos\n\n### fl-go (PUBLIC)\n**Purpose**: Go implementation of OpenFL (Open Framework for Federated Learning). CLI-driven federated learning system where Go handles orchestration and delegates ML operations to Python via gRPC.\n**Tech**: Go (v1.23+), Python (ML), gRPC, React/TypeScript (dashboard)\n**Repo**: https://github.com/fedai-oss/fl-go\n\n**Key Features**:\n- OpenFL-compatible CLI workflow\n- Multi-round and async federated learning (Papaya-based)\n- Aggregation: FedAvg, FedOpt, FedProx\n- gRPC with mTLS support\n- Real-time web monitoring dashboard (React/TypeScript)\n- REST API and WebSocket event streaming\n\n**Key Directories**:\n- `api/` - gRPC protocol definitions\n- `cmd/` - Entry points (aggregator, collaborator, fx CLI, monitor)\n- `pkg/` - Core packages (aggregator, collaborator, federation, monitoring, security)\n- `web/` - React monitoring dashboard\n- `examples/` - Plans, workspaces, sample scripts\n- `docs/` - Documentation\n",
      "chronoIndex": 1
    }
  ],
  "memories": [
    {
      "org": "engram-hq",
      "repo": ".memory",
      "path": "sessions/2026-02-11-platform-scaffold.md",
      "name": "2026-02-11-platform-scaffold.md",
      "content": "---\ndate: \"2026-02-11\"\norg: engram-hq\nrepo: engram-web\ngoal: Scaffold the entire Engram platform from zero to 6 repos with CI\ntype: session\nmodel: claude-opus-4-6\ncost_note: \"shared session with multi-node-raft; see that memory for cost data\"\n---\n\n# Session: Engram Platform Scaffold\n\n**Date**: 2026-02-11\n**Org**: engram-hq\n**Repo**: engram-web, engram-sdk, engram-ios, releases, .skills, roadmap\n**Goal**: Build the Engram visual skill/memory browser from scratch\n\n## Summary\n\nCreated the engram-hq GitHub organization with 6 repositories. Built a full-stack platform: Next.js 15 web app with 15 API endpoints and 9 dashboard pages, TypeScript SDK for agent metrics, SwiftUI iOS app, and a GitHub Pages landing site. All CI pipelines green.\n\n## Phase 1: Web App Foundation\n\n- Initialized Next.js 16 project with Auth.js v5, Drizzle ORM, Turso (SQLite)\n- Designed 10-table schema: users, skills, memories, organizations, repositories, agent_sessions, agent_cost_daily, model_pricing, api_keys, sync_events\n- Built 15 REST API endpoints under /api/v1/\n- ReactMarkdown v10 breaking change: dropped className prop, wrapped in div\n- Edge middleware can't import Node crypto module, used simple NextResponse middleware\n\n## Phase 2: Dashboard UI\n\n- 9 pages: dashboard, skills (tree+list), skill detail, memories timeline, memory detail, organizations, analytics, search, settings\n- All pages wired to real Drizzle queries, not mock data\n- Recharts for analytics visualization (cost trends, model breakdown)\n- shadcn/ui component library with dark theme\n\n## Phase 3: GitHub Sync Engine\n\n- Scans user's GitHub orgs for .skills/ and .memory/ repos\n- Recursive file traversal via GitHub Content API\n- Frontmatter parsing for skill metadata\n- Content hash diffing for incremental sync\n- Tracks sync events with API call counts and duration\n\n## Phase 4: SDK and iOS\n\n- TypeScript SDK (@engram-hq/sdk) with batching, retry, cost tracking\n- SwiftUI iOS app with OAuth, Keychain, Charts, 5-tab navigation\n- Both built from scratch, no templates\n\n## Phase 5: CI and Testing\n\n- engram-web: 4 CI jobs (lint, typecheck, test, build)\n- engram-sdk: 3 CI jobs\n- engram-ios: 2 CI jobs (build+test, lint)\n- Total: 229 tests across all repos\n\n## Key Decisions\n\n- **Turso over Supabase**: SQLite semantics, edge-native, simpler schema\n- **Auth.js v5 over Clerk**: Self-hosted, GitHub-first auth flow\n- **Drizzle over Prisma**: Better SQLite support, type-safe queries, smaller bundle\n- **xcodegen over manual .xcodeproj**: Reproducible project generation, no merge conflicts\n\n## What Failed\n\n- Turso Cloud DB not provisioned (needs browser)\n- GitHub OAuth App not registered (needs browser)\n- iOS simulator runtime not installed locally (8.4GB download)\n",
      "chronoIndex": 1
    },
    {
      "org": "engram-hq",
      "repo": ".memory",
      "path": "sessions/2026-02-12-live-demo-skills.md",
      "name": "2026-02-12-live-demo-skills.md",
      "content": "---\ndate: \"2026-02-12\"\norg: engram-hq\nrepo: releases\ngoal: Build live demo on landing page with real GitHub data\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 48704\noutput_tokens: 6327\ncache_read_tokens: 23628203\ncache_creation_tokens: 1663789\ncost_usd: 28.85\napi_calls: 254\ncost_note: \"equivalent API cost (Max plan)\"\n---\n\n# Session: Live Demo with Real Data\n\n**Date**: 2026-02-12\n**Org**: engram-hq\n**Repo**: releases\n**Goal**: Add a live demo section to the landing page that fetches real skills from public GitHub repos\n\n## Summary\n\nBuilt a fully functional live demo on the Engram landing page that fetches real data from public GitHub repos using the unauthenticated API. Configurable via URL params, client-side markdown rendering, session storage caching.\n\n## The Challenge\n\nAll .skills repos were private. Unauthenticated GitHub API has 60 req/hr rate limit. Needed to show real data without mocking or hardcoding.\n\n## Solution\n\n1. Made engram-hq/.skills repo public (has org-knowledge/SKILL.md)\n2. sreniatnoc/rusd already has a public .skills/ directory\n3. Client-side JavaScript with sessionStorage caching (15min TTL)\n4. URL override via ?orgs=org1,org2 parameter for swappable demo orgs\n\n## Architecture\n\n- DEMO_CONFIG with configurable orgs (default: engram-hq, sreniatnoc)\n- ghFetch() wraps GitHub REST API with caching\n- ghRawFetch() fetches raw file content from raw.githubusercontent.com\n- scanOrgs() discovers .skills repos, .memory repos, and repo-level .skills/ dirs\n- collectMdFiles() does recursive directory traversal\n- renderTree() builds interactive sidebar grouped by org\n- renderMarkdown() client-side markdown to HTML (no deps): frontmatter, code blocks, headers, tables, lists\n- loadFile() fetches and renders a selected skill with metadata badges\n- Auto-selects first skill on page load\n\n## What Worked\n\n- GitHub Content API returns directory listings as JSON arrays\n- raw.githubusercontent.com serves file content without API rate limits\n- sessionStorage prevents redundant API calls within a browsing session\n- Pure JavaScript markdown renderer handles 95% of cases without a library\n\n## What Didn't Work Initially\n\n- GitHub API returns 404 for private repos (expected) - had to make repos public\n- Initial approach tried GitHub Trees API but it requires auth for private repos\n",
      "chronoIndex": 2
    },
    {
      "org": "engram-hq",
      "repo": ".memory",
      "path": "sessions/2026-02-12-production-readiness.md",
      "name": "2026-02-12-production-readiness.md",
      "content": "---\ndate: \"2026-02-12\"\norg: engram-hq\nrepo: engram-web, engram-ios\ngoal: Make both web and iOS apps deployment-ready for real users\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 5444\noutput_tokens: 54407\ncache_read_tokens: 43430430\ncache_creation_tokens: 2447556\ncost_usd: 47.58\napi_calls: 457\ncost_note: \"equivalent API cost (Max plan)\"\n---\n\n# Session: Production Readiness\n\n**Date**: 2026-02-12\n**Org**: engram-hq\n**Repo**: engram-web, engram-ios\n**Goal**: Make both apps fully qualified for local deployment and App Store\n\n## Summary\n\nAdded local SQLite dev mode so the web app works without Turso Cloud. Created auto-migration that sets up all 10 tables on first access. Added Xcode project infrastructure for the iOS app: project.yml, Info.plist, entitlements, privacy manifest, app icon. Both apps now pass CI with full test coverage.\n\n## Phase 1: Web Local Dev Mode\n\n### Problem\nWeb app required Turso Cloud account and GitHub OAuth app just to start locally. No setup docs, no .env.example. Another developer couldn't clone and run.\n\n### Solution\n- Modified lib/db/client.ts to detect file: URLs for local SQLite\n- drizzle.config.ts now auto-switches between sqlite and turso dialects\n- Created setup script (pnpm setup) that creates tables + seeds model pricing\n- Added auto-migration in dashboard layout (ensureMigrated())\n- Created .env.example with documented config\n- Created SETUP.md with 5-minute quickstart guide\n\n### Key Insight\n@libsql/client already supports file: protocol natively. The fix was just conditional config, not a new dependency.\n\n## Phase 2: iOS Xcode Project\n\n### Problem\nApp only had Package.swift (SPM library). No .xcodeproj means no device install, no App Store submission. Missing: app icons, Info.plist, privacy manifest, entitlements.\n\n### Solution\n- Created project.yml for xcodegen (reproducible .xcodeproj generation)\n- Info.plist: Bundle ID com.engram-hq.engram, URL scheme engram://, launch screen\n- Engram.entitlements: Keychain access + App Group\n- PrivacyInfo.xcprivacy: declares UserDefaults, no tracking\n- AppIcon.png: 1024x1024 programmatically generated (purple E on dark background)\n- Updated CI: xcodegen install + generate + xcodebuild build + test on simulator\n\n### What Failed\n- iOS 26.2 simulator runtime not installed locally (8.4GB). Agent tried downloading it, had to abort.\n- Workaround: CI has simulators, verified build+test there. Local testing via Xcode will prompt download.\n\n## Phase 3: Live Demo Enhancement\n\n- Added webhook integration tests (9 tests, PR #5)\n- Added RecCall Tier 4 sync endpoint (PR #7)\n- Added Drizzle migration files (PR #6)\n- Live demo on landing page fetches real skills from public GitHub repos\n\n## Test Coverage\n\n| Repo | Tests | CI Status |\n|------|-------|-----------|\n| engram-web | 149 | Green (4 jobs) |\n| engram-ios | 59 | Green (SPM + Xcode + simulator) |\n| engram-sdk | 21 | Green (3 jobs) |\n| **Total** | **229** | **All green** |\n\n## Lessons Learned\n\n1. tsx (esbuild) doesn't support top-level await in CJS mode - wrap in async main()\n2. xcodegen is the right tool for reproducible iOS projects - keeps .xcodeproj out of git\n3. Free Apple Developer signing expires after 7 days - need paid program for permanent install\n4. Auto-migration at app startup is better UX than requiring a separate setup step\n",
      "chronoIndex": 3
    },
    {
      "org": "sreniatnoc",
      "repo": ".memory",
      "path": "sessions/2026-02-09-rusd-kind-validation.md",
      "name": "2026-02-09-rusd-kind-validation.md",
      "content": "---\ndate: \"2026-02-09\"\norg: sreniatnoc\nrepo: rusd\ngoal: Validate rusd as etcd replacement in Kind K8s cluster\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 2750\noutput_tokens: 98414\ncache_read_tokens: 147402727\ncache_creation_tokens: 5627095\ncost_usd: 132.45\napi_calls: 1513\ncost_note: \"equivalent API cost (Max plan); session spanned Feb 9-10\"\n---\n\n# Session: rusd Kind Cluster Validation\n\n**Date**: 2026-02-09 to 2026-02-10\n**Org**: sreniatnoc\n**Repo**: rusd\n**Goal**: Implement Watch event delivery, fix txn CAS, validate rusd as etcd replacement in Kind K8s cluster\n\n## Summary\n\nImplemented Watch event delivery, txn CAS operations, prev_kv in Watch events, graceful shutdown with serve_with_shutdown(), and HTTP/2 keepalive. Successfully validated rusd as an external etcd backend for a Kind cluster running Kubernetes v1.35. All K8s CRUD operations work including deployments, scaling, rolling updates, pod exec, and namespace lifecycle.\n\n## Phase 1: Initial Validation Attempt\n\nAttempted full validation of rusd for Kubernetes compatibility. Native build and unit tests pass. etcdctl compatibility partial. Kind cluster creation failed due to unimplemented Watch event delivery.\n\n### What Worked\n- cargo build --release succeeds (44 warnings, 0 errors)\n- Docker build succeeds after Dockerfile fixes (167MB image)\n- 40/40 unit tests PASS\n- etcdctl KV (put/get/del), Lease (grant/list/timetolive/revoke), Endpoint health\n\n### What Failed\n- Watch API: acknowledged but never delivered events\n- Kind cluster: kube-apiserver 403 Forbidden (RBAC can't bootstrap without Watch)\n\n## Phase 2: Bugs Found and Fixed\n\n1. **next_revision() returning old value** - returning pre-increment instead of post-increment\n2. **Proto field numbers wrong** - events field was 7 but etcd expects 11, prev_kv missing\n3. **Initial revision 0** - etcd starts at 1, K8s expects >= 1\n4. **Transport error on shutdown** - switched to serve_with_shutdown() for graceful draining\n5. **prev_kv missing in Watch events** - K8s informers need prev_kv for change detection\n6. **Txn compares always succeeding** - implemented actual comparison logic for CAS\n\n## Final State\n\nFull K8s CRUD validated: namespace, configmap, secret, serviceaccount, role, rolebinding, deployment create/scale/rolling-update, service, pod exec. Kind cluster with K8s v1.35 bootstraps and runs all control plane components with rusd as external etcd.\n",
      "chronoIndex": 1
    },
    {
      "org": "sreniatnoc",
      "repo": ".memory",
      "path": "sessions/2026-02-10-production-hardening.md",
      "name": "2026-02-10-production-hardening.md",
      "content": "---\ndate: \"2026-02-10\"\norg: sreniatnoc\nrepo: rusd\ngoal: Add TLS/mTLS, CI pipeline, and API compatibility layer\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 4861\noutput_tokens: 7804\ncache_read_tokens: 8846660\ncache_creation_tokens: 299245\ncost_usd: 7.64\napi_calls: 130\ncost_note: \"equivalent API cost (Max plan); follow-up session to main Feb 10 work\"\n---\n\n# Session: Production Hardening\n\n**Date**: 2026-02-10\n**Org**: sreniatnoc\n**Repo**: rusd\n**Goal**: Make rusd production-grade with TLS, CI, benchmarks, and API compatibility\n\n## Summary\n\nAdded TLS/mTLS support, built a comprehensive CI pipeline with 8 jobs, rewrote integration tests from HTTP to gRPC, added Criterion benchmarks, and achieved 10x faster Kubernetes deploy+scale versus etcd v3.6.7. Merged as PR #1 feat/production-ready.\n\n## Phase 1: TLS/mTLS\n\n- Implemented native TLS using rustls (no OpenSSL dependency)\n- Server-side TLS with PEM cert/key\n- Mutual TLS (mTLS) with client certificate verification\n- Auto-generated test certificates via scripts/gen-certs.sh\n- 8 dedicated TLS tests covering connect, reject, mTLS scenarios\n\n## Phase 2: CI Pipeline\n\nBuilt GitHub Actions CI with 8 parallel jobs:\n- cargo build + clippy lint\n- 48 unit tests\n- 12 integration tests (rewritten from HTTP to gRPC)\n- 8 TLS/mTLS tests\n- 32 Criterion benchmarks\n- Docker build verification\n- K8s validation via Kind cluster (34 test points)\n\nAll 8 jobs green, no continue-on-error.\n\n## Phase 3: Benchmarks\n\nCreated Criterion benchmarks measuring:\n- KV put/get/delete throughput\n- Watch event delivery latency\n- Txn CAS operations per second\n- Prefix scan performance\n\nResults vs etcd v3.6.7:\n- **10x faster** deploy + scale operations\n- **6x faster** rolling updates\n- **2x less memory** at idle\n\n## Lessons Learned\n\n- rustls is simpler than OpenSSL for Rust projects - no system dependency, pure Rust\n- Criterion benchmarks need --bench flag in CI, not cargo test\n- Kind cluster tests need wait_for_leader (election takes ~3s)\n",
      "chronoIndex": 2
    },
    {
      "org": "sreniatnoc",
      "repo": ".memory",
      "path": "sessions/2026-02-11-k8s-test-suite.md",
      "name": "2026-02-11-k8s-test-suite.md",
      "content": "---\ndate: \"2026-02-11\"\norg: sreniatnoc\nrepo: rusd\ngoal: Build comprehensive K8s validation test suite\ntype: session\nmodel: claude-opus-4-6\ncost_note: \"shared session with multi-node-raft; see that memory for cost data\"\n---\n\n# Session: K8s Test Suite\n\n**Date**: 2026-02-11\n**Org**: sreniatnoc\n**Repo**: rusd\n**Goal**: Automated K8s validation with 34 test points in CI\n\n## Summary\n\nBuilt a comprehensive Kubernetes validation script that runs rusd as an external etcd backend for a Kind cluster. The script tests 34 operations covering namespace lifecycle, RBAC, deployments, scaling, rolling updates, services, and pod exec. Runs in CI on every push.\n\n## Test Coverage\n\n34 test points organized into categories:\n- **Namespace**: create, verify, delete (3 tests)\n- **RBAC**: role, rolebinding, serviceaccount CRUD (6 tests)\n- **ConfigMap/Secret**: create, read, update, delete (8 tests)\n- **Deployment**: create, verify pods, scale up/down (6 tests)\n- **Rolling Update**: image update, rollout status, verify (4 tests)\n- **Service**: expose, verify endpoints (3 tests)\n- **Pod Operations**: exec, logs, port-forward (4 tests)\n\n## Key Gotchas\n\n- Kind host.docker.internal works from Docker containers on macOS Docker Desktop\n- etcdctl --count-only is broken in simple/JSON mode, use --keys-only with grep -c\n- CI multi-node tests need wait_for_leader, not just endpoint health\n- grep -c pattern || echo \"0\" causes double output, use || true instead\n",
      "chronoIndex": 3
    },
    {
      "org": "sreniatnoc",
      "repo": ".memory",
      "path": "sessions/2026-02-11-multi-node-raft.md",
      "name": "2026-02-11-multi-node-raft.md",
      "content": "---\ndate: \"2026-02-11\"\norg: sreniatnoc\nrepo: rusd\ngoal: Implement multi-node Raft consensus with real replication\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 28482\noutput_tokens: 19043\ncache_read_tokens: 36971685\ncache_creation_tokens: 1639237\ncost_usd: 35.50\napi_calls: 398\ncost_note: \"equivalent API cost (Max plan); session shared with k8s-test-suite and platform-scaffold\"\n---\n\n# Session: Multi-Node Raft Consensus\n\n**Date**: 2026-02-11\n**Org**: sreniatnoc\n**Repo**: rusd\n**Goal**: Enable multi-node cluster with Raft leader election, log replication, and failover\n\n## Summary\n\nImplemented end-to-end Raft consensus turning rusd from a single-node database into a distributed cluster. Leader election, log replication, heartbeat protocol, and follower forwarding all work. 7 multi-node integration tests pass including leader failover scenarios. Merged as PR #3 feat/e2e-raft-replication.\n\n## Phase 1: Raft Core\n\n- Fixed Send trait issue with parking_lot::Mutex by switching to tokio::sync::Mutex\n- Implemented leader election with randomized timeouts (150-300ms)\n- AppendEntries RPC for log replication with consistency checks\n- RequestVote RPC with term comparison and log freshness checks\n- Heartbeat protocol to maintain leadership\n\n## Phase 2: State Machine Integration\n\n- Raft log entries applied to MVCC storage engine\n- Write operations forwarded from followers to leader via gRPC\n- Leader replicates committed entries to all followers\n- Followers apply entries in order after commit index advances\n\n## Phase 3: Multi-Node Tests\n\n7 integration tests covering:\n1. Three-node cluster formation\n2. Leader election within 5 seconds\n3. Write replication to all nodes\n4. Read consistency across followers\n5. Leader step-down and re-election\n6. Network partition recovery\n7. Stress test with concurrent writes\n\n## Key Decisions\n\n- Used gRPC for inter-node transport (reuses existing tonic infrastructure)\n- Randomized election timeouts prevent split-brain scenarios\n- Follower forwarding is transparent to clients - any node accepts writes\n- CI runs multi-node tests with reduced parallelism (5 streams) for stability\n\n## What's Left\n\n- Snapshot transfer for new node catch-up\n- Dynamic membership changes (AddNode/RemoveNode)\n- Chaos testing with random network partitions\n",
      "chronoIndex": 4
    },
    {
      "org": "ARTIFACTIQ",
      "repo": ".memory",
      "path": "sessions/2026-02-02-e7-shard-training.md",
      "name": "2026-02-02-e7-shard-training.md",
      "content": "---\ndate: \"2026-02-02\"\norg: ARTIFACTIQ\nrepo: ml-models\ngoal: Train E7 model with 46-shard sequential chaining for artifact detection\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 12400\noutput_tokens: 15200\ncache_read_tokens: 18500000\ncache_creation_tokens: 820000\ncost_usd: 16.80\napi_calls: 185\ncost_note: \"equivalent API cost (Max plan)\"\n---\n\n# Session: E7 Shard Training\n\n**Date**: 2026-02-02\n**Org**: ARTIFACTIQ\n**Repo**: ml-models\n**Goal**: Train production YOLOv8 model with 46 shards for 67K+ image artifact detection dataset\n\n## Summary\n\nCompleted E7 training run: 46 shards x 3 epochs each on Apple Silicon MPS. Achieved mAP50 of 0.001454, a 4.6x improvement over E2 baseline. Released as v5.0.0. Discovered critical issue: val=False disabled confidence calibration, producing 1-3% scores instead of 50-90%.\n\n## Phase 1: Training Configuration\n\n- Configured 46 shards x 1500 images each from merged dataset (67,991 training images)\n- Set batch=8, freeze=10, lr0=0.001 with AdamW optimizer\n- Used subprocess isolation per shard for memory cleanup on Apple Silicon\n- Disabled augmentation (mosaic=0.0, mixup=0.0) for stability\n\n## Phase 2: Training Execution\n\n- Full run completed in ~3.5 hours on M-series chip\n- Each shard chains from previous best.pt (sequential fine-tuning)\n- GPU memory stable at 5-6 GB per shard\n- No OOM crashes thanks to subprocess memory isolation\n\n## Phase 3: Validation and Release\n\n- mAP50: 0.001454 (E2 was 0.000316 \u2014 4.6x improvement)\n- Exported to ONNX format (76.8 MB) for cross-platform inference\n- Released as GitHub release v5.0.0 with both .pt and .onnx\n\n## Key Discovery: val=False Bug\n\nSetting val=False during training silently destroys confidence calibration. The model detects objects correctly but assigns absurdly low confidence scores (1-3%). Root cause: without validation passes, the model never learns to calibrate its sigmoid outputs. This is the single most important training parameter.\n\n## Lessons Learned\n\n1. Always set val=True \u2014 confidence calibration requires validation\n2. 3 epochs per shard is critical; 1 epoch (E6) caused regression\n3. ps aux memory values are misleading on Apple Silicon \u2014 use training log GPU stats\n4. Subprocess isolation per shard prevents memory leaks across the full run\n",
      "chronoIndex": 1
    },
    {
      "org": "ARTIFACTIQ",
      "repo": ".memory",
      "path": "sessions/2026-02-03-e8-calibrated-config.md",
      "name": "2026-02-03-e8-calibrated-config.md",
      "content": "---\ndate: \"2026-02-03\"\norg: ARTIFACTIQ\nrepo: ml-models\ngoal: Design E8 calibrated training to fix confidence scores from E7\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 8200\noutput_tokens: 11800\ncache_read_tokens: 14200000\ncache_creation_tokens: 680000\ncost_usd: 12.40\napi_calls: 142\ncost_note: \"equivalent API cost (Max plan)\"\n---\n\n# Session: E8 Calibrated Training Config\n\n**Date**: 2026-02-03\n**Org**: ARTIFACTIQ\n**Repo**: ml-models\n**Goal**: Fix E7's low confidence scores with proper calibration settings\n\n## Summary\n\nCreated e8_calibrated_train.py with correct val=True, reduced freeze layers, proper augmentation, and quality gates. Root cause analysis confirmed val=False destroys confidence calibration. Documented comprehensive memory monitoring guide for Apple Silicon MPS training.\n\n## Phase 1: Root Cause Analysis\n\nIdentified 5 settings that contributed to E7's low confidence:\n- val=False (critical \u2014 no calibration passes)\n- freeze=10 (too aggressive \u2014 detection head can't adapt)\n- mosaic=0.0 (no augmentation leads to overfitting)\n- lr0=0.001 (learning too slow for calibration)\n- patience=0 (no early stopping)\n\n## Phase 2: E8 Configuration\n\nDesigned corrected training config:\n- val=True with mini_val=100 images (6.7% of shard) for speed\n- freeze=5 (less aggressive, lets detection head learn)\n- Disabled mosaic/mixup for E7 fine-tuning stability (uncalibrated weights + augmentation = gradient explosion)\n- lr0=0.001 kept conservative for fine-tuning\n- patience=5 for early stopping\n- cos_lr=True for cosine learning rate schedule\n\n## Phase 3: Memory Monitoring Guide\n\nDocumented accurate memory monitoring for Apple Silicon:\n- ps aux reports misleading values due to shared libraries and MPS pre-allocation\n- Correct method: grep GPU stats from training log\n- MPS typically pre-allocates ~10 GB (normal behavior)\n- Subprocess isolation ensures clean memory per shard\n\n## What's Next\n\n- E9: Run full 46-shard training with E8 settings\n- E10: If E9 shows catastrophic forgetting, implement wavg anchoring\n",
      "chronoIndex": 2
    },
    {
      "org": "ARTIFACTIQ",
      "repo": ".memory",
      "path": "sessions/2026-02-09-sdk-and-mlgpu.md",
      "name": "2026-02-09-sdk-and-mlgpu.md",
      "content": "---\ndate: \"2026-02-09\"\norg: ARTIFACTIQ\nrepo: sdk-python, mlgpu\ngoal: Set up Python SDK and GPU monitoring for ML training workflows\ntype: session\nmodel: claude-opus-4-6\ninput_tokens: 6800\noutput_tokens: 9400\ncache_read_tokens: 11600000\ncache_creation_tokens: 520000\ncost_usd: 10.20\napi_calls: 118\ncost_note: \"equivalent API cost (Max plan)\"\n---\n\n# Session: SDK and mlgpu Setup\n\n**Date**: 2026-02-09\n**Org**: ARTIFACTIQ\n**Repo**: sdk-python, mlgpu\n**Goal**: Establish Python SDK structure and GPU monitor for training workflows\n\n## Summary\n\nSet up the ARTIFACTIQ Python SDK with httpx-based API client for artifact detection, product matching, and tourism package endpoints. Enhanced mlgpu GPU monitor with Apple Silicon MPS support for monitoring YOLO training runs. Created org-level skills and .memory structure.\n\n## Phase 1: Python SDK\n\n- Structured sdk-python with src/artifactiq/ layout\n- API client using httpx with async support\n- Data models for detection results, product matches\n- Three main endpoints: detect, match, tourism-packages\n- pyproject.toml with Python 3.8+ requirement\n\n## Phase 2: mlgpu Enhancements\n\n- Extended GPU monitor (20KB+ Bash script) for Apple Silicon\n- Added framework detection: Create ML, PyTorch, Ultralytics YOLO, HuggingFace\n- CoreML to ONNX converter in tools/ directory\n- Install script for system-wide availability\n\n## Phase 3: Organization Setup\n\n- Created .skills/ with org-knowledge, training skills\n- Created .memory/sessions/ structure for session tracking\n- Documented repo selection guide (releases repo is the website, not website repo)\n- Set up ntfy notifications on artifactiq-ntfy topic\n",
      "chronoIndex": 3
    }
  ],
  "generated_at": "2026-02-15T11:39:06Z",
  "stats": {
    "skills_fetched": 14,
    "skills_cached": 0,
    "skills_failed": 0,
    "memories_fetched": 10,
    "memories_cached": 0,
    "memories_failed": 0
  }
}